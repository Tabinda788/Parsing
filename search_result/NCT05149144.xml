<?xml version="1.0" encoding="UTF-8"?>
<clinical_study rank="922">
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on December 16, 2021</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT05149144</url>
  </required_header>
  <id_info>
    <org_study_id>868</org_study_id>
    <nct_id>NCT05149144</nct_id>
  </id_info>
  <brief_title>Comutti - A Research Project Dedicated to Finding Smart Ways of Using Technology for a Better Tomorrow for Everyone, Everywhere.</brief_title>
  <acronym>COMUTTI</acronym>
  <official_title>Comutti - A Research Project Dedicated to Finding Smart Ways of Using Technology for a Better Tomorrow for Everyone, Everywhere.</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>IRCCS Eugenio Medea</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>Politecnico di Milano</agency>
      <agency_class>Other</agency_class>
    </collaborator>
    <collaborator>
      <agency>Massachusetts Institute of Technology</agency>
      <agency_class>Other</agency_class>
    </collaborator>
  </sponsors>
  <source>IRCCS Eugenio Medea</source>
  <oversight_info>
    <has_dmc>No</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      According to World Health Organization, worldwide one in 160 children has an ASD. About&#xD;
      around 25% to 30% of children are unable to use verbal language to communicate (non-verbal&#xD;
      ASD) or are minimally verbal, i.e., use fewer than 10 words (mv-ASD). The ability to&#xD;
      communicate is a crucial life skill, and difficulties with communication can have a range of&#xD;
      negative consequences such as poorer quality of life and behavioural difficulties.&#xD;
      Communication interventions generally aim to improve children's ability to communicate either&#xD;
      through speech or by supplementing speech with other means (e.g., sign language, pictures, or&#xD;
      AAC - Advanced Augmented Communication tools). Individuals with non- verbal ASD or mv-ASD&#xD;
      often communicate with people through vocalizations that in some cases have a self-consistent&#xD;
      phonetic association to concepts (e.g., &quot;ba&quot; to mean &quot;bathroom&quot;) or are onomatopoeic&#xD;
      expressions (e.g., &quot;woof&quot; to refer to a dog). In most cases vocalizations sound arbitrary;&#xD;
      even if they vary in tone, pitch, and duration depending it is extremely difficult to&#xD;
      interpret the intended message or the individual's emotional or physical state they would&#xD;
      convey, creating a barrier between the persons with ASD and the rest of the world that&#xD;
      originate stress and frustration. Only caregivers who have long term acquaintance with the&#xD;
      subjects are able to decode such wordless sounds and assign them to unique meanings.&#xD;
&#xD;
      This project aims at defining algorithms, methods, and technologies to identify the&#xD;
      communicative intent of vocal expressions generated by children with mv-ASD, and to create&#xD;
      tools that help people who are not familiar with the subjects to understand these individuals&#xD;
      during spontaneous conversations.&#xD;
    </textblock>
  </brief_summary>
  <overall_status>Recruiting</overall_status>
  <start_date type="Actual">July 27, 2021</start_date>
  <completion_date type="Anticipated">December 31, 2022</completion_date>
  <primary_completion_date type="Anticipated">December 31, 2022</primary_completion_date>
  <phase>N/A</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>N/A</allocation>
    <intervention_model>Single Group Assignment</intervention_model>
    <primary_purpose>Basic Science</primary_purpose>
    <masking>None (Open Label)</masking>
  </study_design_info>
  <primary_outcome>
    <measure>Audio signal samples and related labels</measure>
    <time_frame>immediately after the intervention</time_frame>
    <description>The audio signal samples (sounds and verbalizations) produced by each participant are recorded during the hospital stays, in various contexts (i.e., during educational interventions and / or in moments of unstructured play). A small, wireless recorder (Sony TX800 Digital Voice Recorder TX Series) will be attached to the participant's clothing using strong magnets. Next, the adults (caregiver and / or operators) must associate the sounds produced by the child to an affective and / or to the probable meaning of the vocalization -labels- through the use of a web app. Six labels were the same for all users - self-talk, delight, dysregulation, frustration, request, and social exchange.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Participant-specific audio harmonic features identified by supervised/unsupervised machine learning analysis</measure>
    <time_frame>immediately after the intervention</time_frame>
    <description>The collected audio will be segmented in the proximity of the temporal locations of labels. Next, it will be segmented and associated with temporally adjacent labels (affective states or probable meaning of vocalizations). Audio harmonic features (temporal/phonetic characteristics) will be then identified for each participant using supervised/unsupervised machine learning analysis of audio signal samples. Through this process, participant-specific patterns corresponding to specific communications purposes or emotional states will be identified.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Usability/utility of the tool for vocalization interpretation.</measure>
    <time_frame>immediately after the intervention</time_frame>
    <description>The usability/utility of the developed tool for vocalization interpretion will be tested in a retained test set of smartphone-recorded audio files to ascertain the classification accuracy of machine learning analysis.</description>
  </primary_outcome>
  <number_of_arms>1</number_of_arms>
  <enrollment type="Anticipated">25</enrollment>
  <condition>Autism Spectrum Disorder</condition>
  <arm_group>
    <arm_group_label>Experimental: audiosignal dataset creation and machine learning analysis</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Experimental: audiosignal dataset creation and processing; machine learning analysis, empirical evaluations</description>
  </arm_group>
  <intervention>
    <intervention_type>Diagnostic Test</intervention_type>
    <intervention_name>Clinical evaluation of participants by means of Autism Diagnostic Observation Schedule</intervention_name>
    <description>Clinical evaluation of participants by means of Autism Diagnostic Observation Schedule</description>
    <arm_group_label>Experimental: audiosignal dataset creation and machine learning analysis</arm_group_label>
  </intervention>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>audio signal dataset creation and validation; machine learning analysis, empirical evaluations</intervention_name>
    <description>The project tests and adapts the technology developed at MIT for vocalization collection and labeling, and contributes to data gathering among Italian subjects (and their quality validation) in order to create a multi-cultural dataset and to enable cross-cultural studies and analyses. Next, the focus is placed on the analysis of harmonic features of the audio in the vocalizations of the dataset to identify recurring individual features and patterns corresponding to specific communications purposes or emotional states. Supervised and unsupervised machine learning approaches are developed and different machine learning algorithms will be compared to identify the most accurate ones for the project goal. Last, an exploratory evaluation of the vocalization-understanding machine learning model is conducted to test the usability and utility of the tool for vocalization interpretation.</description>
    <arm_group_label>Experimental: audiosignal dataset creation and machine learning analysis</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:&#xD;
&#xD;
          -  having a clinical diagnosis of autism spectrum disorder according to DSM-5 criteria&#xD;
&#xD;
          -  use fewer than 10 words&#xD;
&#xD;
        Exclusion Criteria:&#xD;
&#xD;
          -  using any stimulant or non-stimulant medication affecting the central nervous system&#xD;
&#xD;
          -  having an identified genetic disorder&#xD;
&#xD;
          -  having vision or hearing problems&#xD;
&#xD;
          -  suffering from chronic or acute medical illness&#xD;
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>2 Years</minimum_age>
    <maximum_age>10 Years</maximum_age>
    <healthy_volunteers>No</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>Alessandro Crippa, Ph.D.</last_name>
    <role>Principal Investigator</role>
    <affiliation>IRCCS Eugenio Medea</affiliation>
  </overall_official>
  <overall_contact>
    <last_name>Alessandro Crippa, Ph.D.</last_name>
    <phone>031877593</phone>
    <phone_ext>+39</phone_ext>
    <email>alessandro.crippa@lanostrafamiglia.it</email>
  </overall_contact>
  <location>
    <facility>
      <name>Scientific Institute, IRCCS Eugenio Medea</name>
      <address>
        <city>Bosisio Parini</city>
        <state>Lecco</state>
        <zip>23842</zip>
        <country>Italy</country>
      </address>
    </facility>
    <status>Recruiting</status>
    <contact>
      <last_name>Mariaelena Colombo</last_name>
      <phone>031877357</phone>
      <phone_ext>+39</phone_ext>
      <email>mariaelena.colombo@lanostrafamiglia.it</email>
    </contact>
  </location>
  <location_countries>
    <country>Italy</country>
  </location_countries>
  <verification_date>November 2021</verification_date>
  <study_first_submitted>November 25, 2021</study_first_submitted>
  <study_first_submitted_qc>November 25, 2021</study_first_submitted_qc>
  <study_first_posted type="Actual">December 8, 2021</study_first_posted>
  <last_update_submitted>November 25, 2021</last_update_submitted>
  <last_update_submitted_qc>November 25, 2021</last_update_submitted_qc>
  <last_update_posted type="Actual">December 8, 2021</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>
  <keyword>non-verbal autistic children</keyword>
  <keyword>minimally-verbal autistic children</keyword>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Autism Spectrum Disorder</mesh_term>
  </condition_browse>
  <patient_data>
    <sharing_ipd>No</sharing_ipd>
  </patient_data>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>

